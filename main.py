from langchain_ollama.llms import OllamaLLM;
from langchain_core.prompts import ChatPromptTemplate;
import streamlit as st
import time 

from Core.vector import retriever
from Core.streamlit_setup import init_page, get_user_input

def main():
    #call the streamlit setup function to initialize the page
    init_page()
    
    

    model = OllamaLLM(model="llama3.2")  
    template = """
    You are an exeprt in answering questions about a pizza restaurant

    Here are some relevant reviews: {reviews}

    Here is the question to answer: {question}
    """

    user_input = get_user_input()
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt| model


    if user_input:
        with st.spinner('Generating response...'):
            #reviews are passed into the retriever to get the top 3 most similar reviews from the vector store
            #"invoke" means to call the function and get the result generated by the model
        
            reviews = retriever.invoke(user_input)
            response = chain.invoke({"reviews" : reviews,
                                    "question": user_input})
        
            time.sleep(0.5) 
            
        st.success('Response generated!')
        st.write("**Response:**")
        st.write(response)



if __name__ == "__main__":
    main()